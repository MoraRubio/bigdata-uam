{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWqMtKzFmUIL"
      },
      "source": [
        "# Aprendizaje de Máquina con Big Data y Apache Spark\n",
        "## Exploración y preparación de los datos\n",
        "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
        "\n",
        "Para el procesamiento de Big Data utilizando Ciencia de Datos/Analítica de Datos/Aprendizaje de Máquina, existe una metodología ampliamente utilizada en la industria conocida como CRISP-DM creada por IBM, el siguiente diagrama representa la secuencia de pasos correspondientes a esta metodología:\n",
        "\n",
        "![CRISP-DM](https://www.ibm.com/docs/es/SS3RA7_sub/modeler_crispdm_ddita/clementine/images/crisp_process.jpg)\n",
        "\n",
        "En este diagrama se identifican las siguientes etapas:\n",
        "\n",
        "1. Entendimiento del negocio (objetivos)\n",
        "2. Exploración de los datos\n",
        "3. Preparación de los datos\n",
        "4. Modelado\n",
        "5. Evaluación\n",
        "6. Despliegue\n",
        "\n",
        "El cual representa un proceso iterativo, que comienza con el entendimiento del negocio y termina, y vuelve a comenzar, con la evaluación de resultados.\n",
        "\n",
        "Este notebook está inspirado en el capítulo 2 del libro [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción del problema\n",
        "\n",
        "Usted es el científico de datos de una empresa de bienes raíces en el estado de California de los Estados Unidos de América. Desde hace un tiempo, se ha identificado una gran dificultad a la hora de asignar un precio acertado a una propiedad para ponerla en el mercado, aumentando en gran medida el tiempo que le toma a un agente concretar la venta de la propiedad. En la mayoría de los casos, se ha evidenciado que el precio inicial está por encima del precio real de mercado, lo que hace la propiedad poco atractiva para los compradores; aunque tampoco se descarta que para algunas propiedades se haya listado un precio menor al del mercado, reduciendo el beneficio económico de la compañia y los agentes de venta.\n",
        "\n",
        "En este panorama, se le ha asignado la tarea de predecir el valor promedio de las propiedades en el estado de California a partir de un [conjunto de datos](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) con características relevantes."
      ],
      "metadata": {
        "id": "zurlfySUq_Xt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0o6puVMmUIN"
      },
      "source": [
        "### Configuración del ambiente de Google Colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrlYRkysmUIO"
      },
      "outputs": [],
      "source": [
        "# Download Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Next, we will install Apache Spark 3.0.1 with Hadoop 2.7 from here.\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "# Now, we just need to unzip that folder.\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# Setting JVM and Spark path variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# Installing required packages\n",
        "!pip install pyspark==3.3.2\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "def fetch_housing_data():\n",
        "    tarball_path = Path(\"datasets/housing.tgz\")\n",
        "    if not tarball_path.is_file():\n",
        "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "        urllib.request.urlretrieve(url, tarball_path)\n",
        "        with tarfile.open(tarball_path) as housing_tarball:\n",
        "            housing_tarball.extractall(path=\"datasets\")\n",
        "\n",
        "fetch_housing_data()"
      ],
      "metadata": {
        "id": "nionCeZNnEPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGAEWB7tmUIP"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "import findspark\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pyspark.ml as ml\n",
        "from pyspark.sql import functions as fct\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType, StringType\n",
        "\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt9aC2VlmUIP"
      },
      "source": [
        "### Crear Sesión de Spark e importar los datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ss = (SparkSession\n",
        "      .builder\n",
        "      .appName(\"data_exploration_preparation\")\n",
        "      .getOrCreate())"
      ],
      "metadata": {
        "id": "8Qy3ZQZqo_BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TooSpsLmUIP"
      },
      "outputs": [],
      "source": [
        "path = \"/content/datasets/housing/housing.csv\"\n",
        "housing_data = ss.read.csv(path, inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### División en el conjunto de entrenamiento y conjunto de evaluación"
      ],
      "metadata": {
        "id": "mdksZZSyvUCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.7 # Tamaño del conjunto de entrenamiento: 70%\n",
        "test_size = 0.3 # Tamaño del conjunto de evaluación: 30%\n",
        "housing_data_train, housing_data_test = housing_data.randomSplit([train_size, test_size], seed=42)\n",
        "housing_data_pd = housing_data_train.toPandas() # Convertirlo a un DataFrame de pandas para generar visualizaciones"
      ],
      "metadata": {
        "id": "OpfOIxB3vZrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JYQqS01mUIQ"
      },
      "source": [
        "### Exploración de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_UckRKJmUIR"
      },
      "outputs": [],
      "source": [
        "housing_data_train.printSchema() # Esquema relacional del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_train.show(10) # Primeros 10 registros del conjunto de datos"
      ],
      "metadata": {
        "id": "te5VdYHBs5Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(housing_data_train\n",
        " .describe() # Características estadísticas básicas\n",
        " .show())"
      ],
      "metadata": {
        "id": "x-3u1MSnpE4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gczc3kWcmUIR"
      },
      "outputs": [],
      "source": [
        "housing_data_train.count() # Número de registros"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_pd.hist(bins=50, figsize=(12, 8)) # Calcular y graficar el histograma de cada característica\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-n14qJThuxi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acerca del histograma:\n",
        "- Los ingresos no están representados en dólares (USD), están representados como un valor flotante entre ≃0.5 y ≃15. Al consultar con el equipo que recolectó los datos, este valor está expresado en decenas de miles de dólares, es decir, 3 corresponde a ingresos de aproximadamente \\$30.000 USD.\n",
        "- Los precios de las propiedades tienen un límite artificial de \\$500.000 USD, lo cual puede generar comportamientos indeseados en el modelo, que podría \"entender\" este comportamiento como que no hay propiedades de un mayor valor.\n",
        "- Las distribuciones de la mayoría de las características son asimétricas y no corresponden con una distribución Gaussiana, lo que puede tener un impacto negativo en el desempeño de algunos modelos.\n",
        "- Las características están representadas en escalas y rangos muy diferentes."
      ],
      "metadata": {
        "id": "yHMEzlllXdrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datos geoespaciales"
      ],
      "metadata": {
        "id": "SfeeVjY4NNuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_pd.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZkMAmfadJwLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_pd.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I0uSHJo2NX4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_pd.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
        "                     s=housing_data_pd[\"population\"] / 100, label=\"population\",\n",
        "                     c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
        "                     legend=True, sharex=False, figsize=(10, 7))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A8pdwv1fONAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"california.png\"\n",
        "homl3_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\n",
        "url = homl3_root + \"images/end_to_end_project/\" + filename\n",
        "print(\"Downloading\", filename)\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "housing_renamed = housing_data_pd.rename(columns={\"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
        "                                                  \"population\": \"Population\",\n",
        "                                                  \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\n",
        "\n",
        "housing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\",\n",
        "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
        "                     c=\"Median house value (ᴜsᴅ)\", cmap=\"jet\", colorbar=True,\n",
        "                     legend=True, sharex=False, figsize=(10, 7))\n",
        "\n",
        "california_img = plt.imread(filename)\n",
        "axis = -124.55, -113.95, 32.45, 42.05\n",
        "plt.axis(axis)\n",
        "plt.imshow(california_img, extent=axis)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vE1-VVmDOeeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlaciones\n",
        "La correlación es una medida estadística que expresa hasta qué punto dos variables están relacionadas linealmente (esto es, cambian conjuntamente a una tasa constante). Es una herramienta común para describir relaciones simples sin hacer afirmaciones sobre causa y efecto [[ref](https://www.jmp.com/es_co/statistics-knowledge-portal/what-is-correlation.html)]."
      ],
      "metadata": {
        "id": "arp9NJZ-Qwzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = housing_data_train.columns\n",
        "target_col = \"median_house_value\"\n",
        "cols.remove(\"ocean_proximity\")\n",
        "cols.remove(target_col)\n",
        "print(\"Correlación entre las variables: \")\n",
        "for col in cols:\n",
        "  print(f\"{target_col} - {col}: {housing_data_train.corr(target_col, col)}\")"
      ],
      "metadata": {
        "id": "5IdT49SYQzZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caracteristicas = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
        "pd.plotting.scatter_matrix(housing_data_pd[caracteristicas], figsize=(12, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QyclOMpdSsqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable de mayor correlación con nuestra caractarística objetivo (*median_house_value*) es *median_income*. Fijándonos en el gráfico entre estas dos características evidenciamos una tendencia clara \"hacia arriba\" y poca dispersión entre los puntos."
      ],
      "metadata": {
        "id": "ukemyWTPUtUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_pd.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1, grid=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lM-eSLUlUdZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de nuevos atributos"
      ],
      "metadata": {
        "id": "04IC9qdZZhgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data_train2 = (housing_data_train\n",
        "                      .withColumn(\"rooms_per_household\", housing_data_train[\"total_rooms\"]/housing_data_train[\"households\"])\n",
        "                      .withColumn(\"bedrooms_per_room\", housing_data_train[\"total_bedrooms\"]/housing_data_train[\"total_rooms\"])\n",
        "                      .withColumn(\"population_per_household\", housing_data_train[\"population\"]/housing_data_train[\"households\"]))"
      ],
      "metadata": {
        "id": "xYajtPvNbmya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cols = [\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\"]\n",
        "for col in new_cols:\n",
        "  print(f\"{target_col} - {col}: {housing_data_train2.corr(target_col, col)}\")"
      ],
      "metadata": {
        "id": "zZIEnlWYdDro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparación de los datos"
      ],
      "metadata": {
        "id": "Rvu-otMndiZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Separación de características/predictores y etiquetas"
      ],
      "metadata": {
        "id": "zn-QBOjHdwzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = housing_data_train.drop(\"median_house_value\")\n",
        "y_train = housing_data_train.select(\"median_house_value\")\n",
        "\n",
        "X_test = housing_data_test.drop(\"median_house_value\")\n",
        "y_test = housing_data_test.select(\"median_house_value\")"
      ],
      "metadata": {
        "id": "CrheWRfsdmYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tratamiento de datos nulos"
      ],
      "metadata": {
        "id": "iT_FLVsEe6Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train\n",
        " .describe()\n",
        " .show())"
      ],
      "metadata": {
        "id": "GF173vqIfeiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opción 1: descartar los registros con datos nulos"
      ],
      "metadata": {
        "id": "nRiSDL4Bfv_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_valid = X_train.dropna(how = \"any\", subset = [\"total_bedrooms\"])\n",
        "X_train_valid.count()"
      ],
      "metadata": {
        "id": "nIiBX9YSfvc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opción 2: descartar el atributo"
      ],
      "metadata": {
        "id": "rhH96Ha5hIl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ = X_train.drop(\"total_bedrooms\")\n",
        "print(X_train_.count())\n",
        "print(X_train.columns)"
      ],
      "metadata": {
        "id": "kH36bPXfhLB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opción 3: al ser una característica numérica, se pueden reemplazar los valores faltantes con alguna medida estadística de esa característica."
      ],
      "metadata": {
        "id": "WWEiwRJ3f-2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = (ml.feature.Imputer()\n",
        ".setStrategy(\"median\")\n",
        ".setInputCols([\"total_bedrooms\"])\n",
        ".setOutputCols([\"total_bedrooms_complete\"]))\n",
        "model = imputer.fit(X_train)\n",
        "model.surrogateDF.show()"
      ],
      "metadata": {
        "id": "Cir_MNQafG0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_median = model.transform(X_train)\n",
        "X_train_median.count()"
      ],
      "metadata": {
        "id": "lhaIYLf_g9qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Características categóricas\n",
        "De manera general, los modelos o algoritmos de aprendizaje de máquina solo pueden ser entrenados con características numéricas. Las características de texto o categóricas deben ser convertidas a una representación numérica."
      ],
      "metadata": {
        "id": "LbC3SIY1hdVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.select(\"ocean_proximity\").show(5)"
      ],
      "metadata": {
        "id": "v0UWbsxgh1PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_ocean_prox = (X_train\n",
        "                     .select(\"ocean_proximity\")\n",
        "                     .dropDuplicates()\n",
        "                     .collect())\n",
        "unique_ocean_prox = [item.ocean_proximity for item in unique_ocean_prox]\n",
        "print(unique_ocean_prox)"
      ],
      "metadata": {
        "id": "ZtbxUQeTkx9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opción 1: reemplazar los valores con un número entero"
      ],
      "metadata": {
        "id": "S-H1UgVqlOqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_dict = {item: i for i, item in enumerate(unique_ocean_prox)}\n",
        "print(map_dict)"
      ],
      "metadata": {
        "id": "UL3DJRqOlNT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stringIndexer = ml.feature.StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ordinal_ocean_proximity\", stringOrderType=\"frequencyDesc\")\n",
        "stringIndexer.setHandleInvalid(\"error\")\n",
        "model = stringIndexer.fit(X_train)"
      ],
      "metadata": {
        "id": "tLxzb6al9TZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ordinal = model.transform(X_train).drop(\"ocean_proximity\")\n",
        "X_train_ordinal.show()"
      ],
      "metadata": {
        "id": "8KHoEZSw6gV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opción 2: One-hot encoding\n",
        "\n",
        "Esta segunda opción busca evitar un problema que se presenta con la opción 1 que es la de asignar implícitamente una jerarquía u orden a nuestra variable categórica, lo que en muchas ocasiones no representa la realidad."
      ],
      "metadata": {
        "id": "3BHkWbtW6yaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@fct.udf(returnType=StringType())\n",
        "def one_hot_encoding(x, map_dict = map_dict):\n",
        "  values = list(map_dict.keys())\n",
        "  one_hot_vector = np.zeros(len(values), dtype=np.uint8)\n",
        "  one_hot_vector[map_dict.get(x)] = 1\n",
        "  return ','.join([str(i) for i in one_hot_vector])"
      ],
      "metadata": {
        "id": "oOnU9fn-7Ox7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train\n",
        " .withColumn(\"onehot_ocean_proximity\", one_hot_encoding(X_train.ocean_proximity))\n",
        " .withColumn(\"onehot_ocean_ISLAND\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(0))\n",
        " .withColumn(\"onehot_ocean_NEAR_OCEAN\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(1))\n",
        " .withColumn(\"onehot_ocean_NEAR_BAY\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(2))\n",
        " .withColumn(\"onehot_ocean_1H_OCEAN\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(3))\n",
        " .withColumn(\"onehot_ocean_INLAND\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(4))\n",
        " #.where(\"ocean_proximity = 'NEAR BAY'\")\n",
        " .show())"
      ],
      "metadata": {
        "id": "KtT5xZp19JTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehotencoder = ml.feature.OneHotEncoder(inputCol=\"ordinal_ocean_proximity\", outputCol=\"onehot_ocean_proximity\")\n",
        "onehotencoder.setHandleInvalid(\"error\")\n",
        "model = onehotencoder.fit(X_train_ordinal)"
      ],
      "metadata": {
        "id": "sXM54ols-NdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_onehot = model.transform(X_train_ordinal).drop(\"ordinal_ocean_proximity\")\n",
        "X_train_onehot.show()"
      ],
      "metadata": {
        "id": "esHY84wH-v6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_onehot = (X_train\n",
        "                  .withColumn(\"onehot_ocean_proximity\", one_hot_encoding(X_train.ocean_proximity))\n",
        "                  .withColumn(\"onehot_ocean_ISLAND\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(0).cast(IntegerType()))\n",
        "                  .withColumn(\"onehot_ocean_NEAR_OCEAN\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(1).cast(IntegerType()))\n",
        "                  .withColumn(\"onehot_ocean_NEAR_BAY\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(2).cast(IntegerType()))\n",
        "                  .withColumn(\"onehot_ocean_1H_OCEAN\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(3).cast(IntegerType()))\n",
        "                  .withColumn(\"onehot_ocean_INLAND\", fct.split(\"onehot_ocean_proximity\", \",\").getItem(4).cast(IntegerType()))\n",
        "                  .drop(\"ocean_proximity\")\n",
        "                  .drop(\"onehot_ocean_proximity\"))\n",
        "X_train_onehot.show()"
      ],
      "metadata": {
        "id": "1_BuKtwAAC0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalización"
      ],
      "metadata": {
        "id": "DYrBf5mIF0VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_minmax_scaled = X_train_valid.alias(\"X_train_minmax_scaled\")\n",
        "numeric_cols = [\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\"]\n",
        "vec_numeric_cols = [\"vec_\"+col for col in numeric_cols]\n",
        "vecsAssembler = []\n",
        "for in_col, out_col in zip(numeric_cols, vec_numeric_cols):\n",
        "  vecAssembler = ml.feature.VectorAssembler(outputCol=out_col, inputCols=[in_col])\n",
        "  X_train_minmax_scaled = vecAssembler.transform(X_train_minmax_scaled)\n",
        "  vecsAssembler.append(vecAssembler)\n",
        "X_train_minmax_scaled.show()"
      ],
      "metadata": {
        "id": "ObLAQCAsPVKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_numeric_cols = [\"scaled_\"+col for col in numeric_cols]\n",
        "minmaxScalers = []\n",
        "for in_col, out_col in zip(vec_numeric_cols, out_numeric_cols):\n",
        "  minmaxscaler = ml.feature.MinMaxScaler(outputCol=out_col, inputCol=in_col)\n",
        "  model = minmaxscaler.fit(X_train_minmax_scaled)\n",
        "  minmaxScalers.append(model)\n",
        "  X_train_minmax_scaled = model.transform(X_train_minmax_scaled)\n",
        "X_train_minmax_scaled.show()"
      ],
      "metadata": {
        "id": "Bx8U_QLWF0Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_minmax_scaled = X_train_minmax_scaled.select('scaled_longitude', 'scaled_latitude', 'scaled_housing_median_age',\n",
        "                                              'scaled_total_rooms', 'scaled_total_bedrooms', 'scaled_population',\n",
        "                                              'scaled_households', 'scaled_median_income')\n",
        "X_train_minmax_scaled.show()"
      ],
      "metadata": {
        "id": "CDemzhHm6oy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creación del Pipeline (Secuencia de operaciones)\n",
        "\n",
        "<!--\n",
        "\n",
        "sqlTrans = ml.feature.SQLTransformer(statement=\"SELECT scaled_longitude AS longitude, \\\n",
        "                                                       scaled_latitude AS latitude, \\\n",
        "                                                       scaled_housing_median_age AS housing_median_age, \\\n",
        "                                                       scaled_total_rooms AS total_rooms, \\\n",
        "                                                       scaled_total_bedrooms_complete AS total_bedrooms, \\\n",
        "                                                       scaled_population AS population, \\\n",
        "                                                       scaled_households AS households, \\\n",
        "                                                       scaled_median_income AS median_income, \\\n",
        "                                                       onehot_ocean_proximity AS ocean_proximity, \\\n",
        "                                                       features, \\\n",
        "                                                       median_house_value AS label FROM __THIS__\")\n",
        "\n",
        "\n",
        "-->"
      ],
      "metadata": {
        "id": "Q2x89ygI7Xi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = ml.feature.Imputer(strategy=\"median\",inputCols=[\"total_bedrooms\"],outputCols=[\"total_bedrooms_complete\"])\n",
        "\n",
        "stringIndexer = ml.feature.StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ordinal_ocean_proximity\", stringOrderType=\"frequencyDesc\")\n",
        "\n",
        "onehotencoder = ml.feature.OneHotEncoder(inputCol=\"ordinal_ocean_proximity\", outputCol=\"onehot_ocean_proximity\")\n",
        "\n",
        "columns_to_scale = [\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms_complete\",\"population\",\"households\",\"median_income\"]\n",
        "\n",
        "assemblers = [ml.feature.VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale+[\"median_house_value\"]]\n",
        "\n",
        "scalers = [ml.feature.MinMaxScaler(inputCol=col + \"_vec\", outputCol=\"scaled_\" + col) for col in columns_to_scale]\n",
        "\n",
        "feature_assembler = ml.feature.VectorAssembler(inputCols=[\"scaled_\" + col for col in columns_to_scale]+[\"onehot_ocean_proximity\"], outputCol=\"features\")\n",
        "\n",
        "sqlTrans = ml.feature.SQLTransformer(statement=\"SELECT features, median_house_value_vec AS label FROM __THIS__\")\n",
        "\n",
        "preprocess_pipeline = ml.Pipeline(stages=[imputer, stringIndexer, onehotencoder]+assemblers+scalers+[feature_assembler, sqlTrans])"
      ],
      "metadata": {
        "id": "mxTTDhNN7XPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_model = preprocess_pipeline.fit(housing_data_train)\n",
        "pipeline_model.transform(housing_data_train).show()"
      ],
      "metadata": {
        "id": "cOX_tRLFALTL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}