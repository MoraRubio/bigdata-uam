{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EWqMtKzFmUIL"
      },
      "source": [
        "# Aprendizaje de Máquina con Big Data y Apache Spark\n",
        "## Entrenamiento de modelos y evaluación\n",
        "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
        "\n",
        "Para el procesamiento de Big Data utilizando Ciencia de Datos/Analítica de Datos/Aprendizaje de Máquina, existe una metodología ampliamente utilizada en la industria conocida como CRISP-DM creada por IBM, el siguiente diagrama representa la secuencia de pasos correspondientes a esta metodología:\n",
        "\n",
        "![CRISP-DM](https://www.ibm.com/docs/es/SS3RA7_sub/modeler_crispdm_ddita/clementine/images/crisp_process.jpg)\n",
        "\n",
        "En este diagrama se identifican las siguientes etapas:\n",
        "\n",
        "1. Entendimiento del negocio (objetivos)\n",
        "2. Exploración de los datos\n",
        "3. Preparación de los datos\n",
        "4. Modelado\n",
        "5. Evaluación\n",
        "6. Despliegue\n",
        "\n",
        "El cual representa un proceso iterativo, que comienza con el entendimiento del negocio y termina, y vuelve a comenzar, con la evaluación de resultados.\n",
        "\n",
        "Este notebook está inspirado en el capítulo 2 del libro [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zurlfySUq_Xt"
      },
      "source": [
        "## Descripción del problema\n",
        "\n",
        "Usted es el científico de datos de una empresa de bienes raíces en el estado de California de los Estados Unidos de América. Desde hace un tiempo, se ha identificado una gran dificultad a la hora de asignar un precio acertado a una propiedad para ponerla en el mercado, aumentando en gran medida el tiempo que le toma a un agente concretar la venta de la propiedad. En la mayoría de los casos, se ha evidenciado que el precio inicial está por encima del precio real de mercado, lo que hace la propiedad poco atractiva para los compradores; aunque tampoco se descarta que para algunas propiedades se haya listado un precio menor al del mercado, reduciendo el beneficio económico de la compañia y los agentes de venta.\n",
        "\n",
        "En este panorama, se le ha asignado la tarea de predecir el valor promedio de las propiedades en el estado de California a partir de un [conjunto de datos](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) con características relevantes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m0o6puVMmUIN"
      },
      "source": [
        "### Configuración del ambiente de Google Colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrlYRkysmUIO"
      },
      "outputs": [],
      "source": [
        "# Download Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Next, we will install Apache Spark 3.0.1 with Hadoop 2.7 from here.\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "# Now, we just need to unzip that folder.\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# Setting JVM and Spark path variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# Installing required packages\n",
        "!pip install pyspark==3.3.2\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nionCeZNnEPP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "def fetch_housing_data():\n",
        "    tarball_path = Path(\"datasets/housing.tgz\")\n",
        "    if not tarball_path.is_file():\n",
        "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "        urllib.request.urlretrieve(url, tarball_path)\n",
        "        with tarfile.open(tarball_path) as housing_tarball:\n",
        "            housing_tarball.extractall(path=\"datasets\")\n",
        "\n",
        "fetch_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AGAEWB7tmUIP"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "import findspark\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pyspark.ml as ml\n",
        "from pyspark.sql import functions as fct\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType, StringType\n",
        "\n",
        "findspark.init()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pt9aC2VlmUIP"
      },
      "source": [
        "### Crear Sesión de Spark e importar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8Qy3ZQZqo_BW"
      },
      "outputs": [],
      "source": [
        "ss = (SparkSession\n",
        "      .builder\n",
        "      .appName(\"data_exploration_preparation\")\n",
        "      .getOrCreate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_TooSpsLmUIP"
      },
      "outputs": [],
      "source": [
        "path = \"/content/datasets/housing/housing.csv\"\n",
        "housing_data = ss.read.csv(path, inferSchema=True, header=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mdksZZSyvUCM"
      },
      "source": [
        "### División en el conjunto de entrenamiento y conjunto de evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OpfOIxB3vZrM"
      },
      "outputs": [],
      "source": [
        "train_size = 0.7 # Tamaño del conjunto de entrenamiento: 70%\n",
        "test_size = 0.3 # Tamaño del conjunto de evaluación: 30%\n",
        "housing_data_train, housing_data_test = housing_data.randomSplit([train_size, test_size], seed=42)\n",
        "housing_data_pd = housing_data_train.toPandas() # Convertirlo a un DataFrame de pandas para generar visualizaciones"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2x89ygI7Xi4"
      },
      "source": [
        "### Creación del Pipeline de preprocesamiento de los datos\n",
        "\n",
        "<!--\n",
        "\n",
        "sqlTrans = ml.feature.SQLTransformer(statement=\"SELECT scaled_longitude AS longitude, \\\n",
        "                                                       scaled_latitude AS latitude, \\\n",
        "                                                       scaled_housing_median_age AS housing_median_age, \\\n",
        "                                                       scaled_total_rooms AS total_rooms, \\\n",
        "                                                       scaled_total_bedrooms_complete AS total_bedrooms, \\\n",
        "                                                       scaled_population AS population, \\\n",
        "                                                       scaled_households AS households, \\\n",
        "                                                       scaled_median_income AS median_income, \\\n",
        "                                                       onehot_ocean_proximity AS ocean_proximity, \\\n",
        "                                                       features, \\\n",
        "                                                       median_house_value AS label FROM __THIS__\")\n",
        "\n",
        "\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxTTDhNN7XPk"
      },
      "outputs": [],
      "source": [
        "imputer = ml.feature.Imputer(strategy=\"median\",inputCols=[\"total_bedrooms\"],outputCols=[\"total_bedrooms_complete\"])\n",
        "\n",
        "stringIndexer = ml.feature.StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ordinal_ocean_proximity\", stringOrderType=\"frequencyDesc\")\n",
        "\n",
        "onehotencoder = ml.feature.OneHotEncoder(inputCol=\"ordinal_ocean_proximity\", outputCol=\"onehot_ocean_proximity\")\n",
        "\n",
        "columns_to_scale = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms_complete\", \"population\", \"households\", \"median_income\"]\n",
        "\n",
        "assemblers = [ml.feature.VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]#+[\"median_house_value\"]]\n",
        "\n",
        "scalers = [ml.feature.MinMaxScaler(inputCol=col + \"_vec\", outputCol=\"scaled_\" + col) for col in columns_to_scale]\n",
        "\n",
        "feature_assembler = ml.feature.VectorAssembler(inputCols=[\"scaled_\" + col for col in columns_to_scale]+[\"onehot_ocean_proximity\"], outputCol=\"features\")\n",
        "\n",
        "sqlTrans = ml.feature.SQLTransformer(statement=\"SELECT features, median_house_value AS label FROM __THIS__\")\n",
        "\n",
        "preprocess_pipeline = ml.Pipeline(stages=[imputer, stringIndexer, onehotencoder]+assemblers+scalers+[feature_assembler, sqlTrans])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOX_tRLFALTL"
      },
      "outputs": [],
      "source": [
        "pipeline_model = preprocess_pipeline.fit(housing_data_train)\n",
        "pipeline_model.transform(housing_data_train).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki_4GnWzQePb"
      },
      "source": [
        "### Entrenamiento y evaluación de modelos\n",
        "\n",
        "*Entrenar* un modelo se refiere a configurar sus parámetros de forma que se ajusten de la mejor manera a los datos del conjunto de entrenamiento. Para esto se hace necesario definir una medida que nos indique qué tan bien (o mal) se está realizando ese ajuste de los parámetros, y al final que conjunto de parámetros obtiene el mejor resultado; nos referimos a esta medida como **métrica** o **función de desempeño**. Para tareas de regresión, la métrica más utilizada es la raíz cuadrada del error cuadrático medio o RMSE (*Root Mean Squared Error*), definido como:\n",
        "\n",
        "$$RMSE(X,\\theta)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})^2}$$\n",
        "\n",
        "Donde $X$ representa nuestro conjunto de entrenamiento con $m$ muestras, $x^{(i)}$ representa los valores de las características de entrada para una muestra y $y^{(i)}$ la etiqueta o valor objetivo; y $\\theta$ representa los parámetros del modelo. $\\theta^Tx^{(i)}$ corresponde a las predicciones del modelo, por lo que también lo podríamos expresar como $\\hat y$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qTqaWA40aEBF"
      },
      "source": [
        "#### Modelo de regresión lineal\n",
        "\n",
        "La regresión lineal consiste en ajustar una línea recta o un plano a los datos para realizar predicciones de un valor númerico. Para una variable, este modelo se puede expresar matemáticamente como:\n",
        "\n",
        "$$\\hat y=\\theta_0+\\theta_1x$$\n",
        "\n",
        "Lo cual podemos interpretar como un modelo donde la predicción es una suma ponderada de las variables de entrada.\n",
        "\n",
        "Generalizando para $n$ variables:\n",
        "\n",
        "$$\\hat y=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$$\n",
        "\n",
        "Donde:\n",
        "- $\\hat y$ es la predicción\n",
        "- $n$ es el número de variables o características\n",
        "- $x_i$ es el valor de la característica $i$-ésima\n",
        "- $\\theta_j$ es el $j$-ésimo parámetro del modelo, incluyendo el intercepto $\\theta_0$ y los pesos de las características"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cvRpjNZQZUF"
      },
      "outputs": [],
      "source": [
        "linear_regression = ml.regression.LinearRegression(maxIter=1000)\n",
        "model_pipeline = ml.Pipeline(stages=[preprocess_pipeline, linear_regression])\n",
        "\n",
        "model = model_pipeline.fit(housing_data_train)\n",
        "\n",
        "model.transform(housing_data_train).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgXgIqFkSRbJ"
      },
      "outputs": [],
      "source": [
        "# Obtener el modelo a partir del pipeline\n",
        "lrModel = model.stages[-1]\n",
        "\n",
        "# Imprimir los pesos y el intercepto para el modelo de regresión lineal\n",
        "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
        "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
        "\n",
        "# Resumen del modelo y algunas métricas sobre los datos de entrenamiento\n",
        "trainingSummary = lrModel.summary\n",
        "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"r2: %f\" % trainingSummary.r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KbAqmPxRxYM"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(housing_data_test)\n",
        "predictions.show(5)\n",
        "evaluator = ml.evaluation.RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zdSmA6dXaHwp"
      },
      "source": [
        "#### Modelo de árboles de decisión\n",
        "\n",
        "Un árbol de decisión es un algoritmo de aprendizaje supervisado no paramétrico, que se utiliza tanto para tareas de clasificación como de regresión. Tiene una estructura de árbol jerárquica, que consta de un nodo raíz, ramas, nodos internos y nodos hoja [[1]](https://www.ibm.com/es-es/topics/decision-trees).\n",
        "\n",
        "![Árbol de decisión](https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/cdp/cf/ul/g/df/de/Decision-Tree.component.xl.ts=1666031329586.png/content/adobe-cms/es/es/topics/decision-trees/jcr:content/root/table_of_contents/intro/complex_narrative/items/content_group_1423241468/image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txrK9Q7aUaf1"
      },
      "outputs": [],
      "source": [
        "decisionTree = ml.regression.DecisionTreeRegressor()\n",
        "model_pipeline = ml.Pipeline(stages=[preprocess_pipeline, decisionTree])\n",
        "\n",
        "model = model_pipeline.fit(housing_data_train)\n",
        "\n",
        "train_predictions = model.transform(housing_data_train)\n",
        "train_predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7ajiGTjUaf3"
      },
      "outputs": [],
      "source": [
        "print(model.stages[-1].featureImportances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYoYw6ERU-QM"
      },
      "outputs": [],
      "source": [
        "evaluator = ml.evaluation.RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(train_predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on train data = %g\" % rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9lUANo0Uaf4"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(housing_data_test)\n",
        "predictions.show(5)\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "knX19UDdaL1j"
      },
      "source": [
        "#### Modelos de ensamble: Random Forest\n",
        "\n",
        "El random forest es un algoritmo de machine learning de uso común registrado por Leo Breiman y Adele Cutler, que combina la salida de múltiples árboles de decisión para alcanzar un solo resultado. Su facilidad de uso y flexibilidad han impulsado su adopción, ya que maneja problemas de clasificación y regresión. \n",
        "\n",
        "El algoritmo de random forest se compone de un conjunto de árboles de decisión, y cada árbol del conjunto se compone de una muestra de datos extraída de un conjunto de entrenamiento con reemplazo. Utiliza la aleatoriedad de características para crear un bosque no correlacionado de árboles de decisión [[2]](https://www.ibm.com/es-es/topics/random-forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUZQzOZQV1rl"
      },
      "outputs": [],
      "source": [
        "randomForest = ml.regression.RandomForestRegressor()\n",
        "model_pipeline = ml.Pipeline(stages=[preprocess_pipeline, randomForest])\n",
        "\n",
        "model = model_pipeline.fit(housing_data_train)\n",
        "\n",
        "train_predictions = model.transform(housing_data_train)\n",
        "train_predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzIGouLXV1ru"
      },
      "outputs": [],
      "source": [
        "print(model.stages[-1].featureImportances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nxWrQ3OV1ru"
      },
      "outputs": [],
      "source": [
        "evaluator = ml.evaluation.RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(train_predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on train data = %g\" % rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF-dUlxxV1rv"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(housing_data_test)\n",
        "predictions.show(5)\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zaJe-QM_aY38"
      },
      "source": [
        "### Ajuste de los modelos\n",
        "\n",
        "La siguiente celda tarda en ejecutar alrededor de 16 minutos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mc3hD10vao_M"
      },
      "outputs": [],
      "source": [
        "paramGrid = (ml.tuning.ParamGridBuilder()\n",
        "             .addGrid(imputer.strategy, [\"mean\", \"mode\"])\n",
        "             .addGrid(randomForest.numTrees, [20, 50, 100])\n",
        "             .addGrid(randomForest.maxDepth, [5, 10, 15])\n",
        "             .build())\n",
        "\n",
        "crossval = ml.tuning.CrossValidator(estimator=model_pipeline,\n",
        "                                    estimatorParamMaps=paramGrid,\n",
        "                                    evaluator=ml.evaluation.RegressionEvaluator(metricName=\"rmse\"),\n",
        "                                    numFolds=2)\n",
        "\n",
        "# Run cross-validation, and choose the best set of parameters.\n",
        "cvModel = crossval.fit(housing_data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1dStYkwCrZf",
        "outputId": "2f93a802-508b-4bc8-a699-b38b03909857"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[69021.84464164326,\n",
              " 57870.5792590263,\n",
              " 55793.25891993652,\n",
              " 68346.14602693476,\n",
              " 56972.30227252983,\n",
              " 54477.309875511855,\n",
              " 68615.760966357,\n",
              " 57011.33817286771,\n",
              " 54332.94862000374,\n",
              " 69007.19441872317,\n",
              " 57790.46496921142,\n",
              " 55603.95241932221,\n",
              " 68395.882071058,\n",
              " 56930.328801938944,\n",
              " 54455.43365359685,\n",
              " 68518.87799900028,\n",
              " 56833.62248913152,\n",
              " 54177.17681773282]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cvModel.avgMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6X9o0_3C0Vq",
        "outputId": "7e67d180-a43e-4dce-a13d-9d7cf19380bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[347.8015940742116,\n",
              " 405.41841137834126,\n",
              " 715.1965414721162,\n",
              " 443.35799159102316,\n",
              " 770.5886910378322,\n",
              " 716.4265209351761,\n",
              " 369.6458446341421,\n",
              " 429.1892733001878,\n",
              " 543.1629224696808,\n",
              " 354.79779027934273,\n",
              " 221.39009861246086,\n",
              " 506.86581369236956,\n",
              " 501.61918342389254,\n",
              " 502.74413407020256,\n",
              " 596.8556373187093,\n",
              " 265.49832042490743,\n",
              " 297.4278743308023,\n",
              " 528.8094796218829]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cvModel.stdMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-Z1vz4r8Hi4",
        "outputId": "397cffef-b4d2-4241-ea40-3c98933cac9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{Param(parent='Imputer_3f27637fec74', name='strategy', doc='strategy for imputation. If mean, then replace missing values using the mean value of the feature. If median, then replace missing values using the median value of the feature. If mode, then replace missing using the most frequent value of the feature.'): 'mean',\n",
              " Param(parent='RandomForestRegressor_5a46af05f3fd', name='numTrees', doc='Number of trees to train (>= 1).'): 20,\n",
              " Param(parent='RandomForestRegressor_5a46af05f3fd', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cvModel.getEstimatorParamMaps()[np.argmax(cvModel.avgMetrics)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ys_1F7LXDDaM"
      },
      "outputs": [],
      "source": [
        "ModelsParams = []\n",
        "for modelsParams in cvModel.getEstimatorParamMaps():\n",
        "  params = []\n",
        "  for key, item in modelsParams.items():\n",
        "    params.append(f\"{key.name}: {item}\")\n",
        "  ModelsParams.append(\" - \".join(params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN7SAgGVD1Mc",
        "outputId": "605db76b-ff91-46e6-dca7-7da1af3368bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Para los parámetros strategy: mean - numTrees: 20 - maxDepth: 5: \n",
            " Se obtuvo un rmse de 69021.84464164326 ± 347.8015940742116 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 20 - maxDepth: 10: \n",
            " Se obtuvo un rmse de 57870.5792590263 ± 405.41841137834126 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 20 - maxDepth: 15: \n",
            " Se obtuvo un rmse de 55793.25891993652 ± 715.1965414721162 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 50 - maxDepth: 5: \n",
            " Se obtuvo un rmse de 68346.14602693476 ± 443.35799159102316 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 50 - maxDepth: 10: \n",
            " Se obtuvo un rmse de 56972.30227252983 ± 770.5886910378322 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 50 - maxDepth: 15: \n",
            " Se obtuvo un rmse de 54477.309875511855 ± 716.4265209351761 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 100 - maxDepth: 5: \n",
            " Se obtuvo un rmse de 68615.760966357 ± 369.6458446341421 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 100 - maxDepth: 10: \n",
            " Se obtuvo un rmse de 57011.33817286771 ± 429.1892733001878 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mean - numTrees: 100 - maxDepth: 15: \n",
            " Se obtuvo un rmse de 54332.94862000374 ± 543.1629224696808 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 20 - maxDepth: 5: \n",
            " Se obtuvo un rmse de 69007.19441872317 ± 354.79779027934273 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 20 - maxDepth: 10: \n",
            " Se obtuvo un rmse de 57790.46496921142 ± 221.39009861246086 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 20 - maxDepth: 15: \n",
            " Se obtuvo un rmse de 55603.95241932221 ± 506.86581369236956 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 50 - maxDepth: 5: \n",
            " Se obtuvo un rmse de 68395.882071058 ± 501.61918342389254 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 50 - maxDepth: 10: \n",
            " Se obtuvo un rmse de 56930.328801938944 ± 502.74413407020256 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 50 - maxDepth: 15: \n",
            " Se obtuvo un rmse de 54455.43365359685 ± 596.8556373187093 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 100 - maxDepth: 5: \n",
            " Se obtuvo un rmse de 68518.87799900028 ± 265.49832042490743 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 100 - maxDepth: 10: \n",
            " Se obtuvo un rmse de 56833.62248913152 ± 297.4278743308023 \n",
            "\n",
            "\n",
            "Para los parámetros strategy: mode - numTrees: 100 - maxDepth: 15: \n",
            " Se obtuvo un rmse de 54177.17681773282 ± 528.8094796218829 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for params, mean, std in zip(ModelsParams, cvModel.avgMetrics, cvModel.stdMetrics):\n",
        "  print(f\"Para los parámetros {params}: \\n Se obtuvo un rmse de {mean} ± {std} \\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3p2pft979Bc",
        "outputId": "7491d51c-4db5-4b35-f96a-53540f676573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+------------------+\n",
            "|            features|   label|        prediction|\n",
            "+--------------------+--------+------------------+\n",
            "|[0.00498007968127...|103600.0|109501.29450353034|\n",
            "|[0.01195219123505...|106700.0|116568.15092823739|\n",
            "|[0.01195219123505...| 73200.0|105368.09296504149|\n",
            "|[0.01294820717131...| 78300.0|  82654.6257902175|\n",
            "|[0.01593625498007...| 90100.0|162644.31522776833|\n",
            "+--------------------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 51640.5\n"
          ]
        }
      ],
      "source": [
        "prediction = cvModel.transform(housing_data_test)\n",
        "prediction.show(5)\n",
        "rmse = evaluator.evaluate(prediction)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uL7Ed5_UapoT"
      },
      "source": [
        "### Despliegue\n",
        "\n",
        "La etapa de despliegue se refiere a poner nuestro modelo en producción, sea creando un script que recupere los datos de nuestro sistema de almacenamiento, o mediante la creación de una API (_Application Programming Interface_) que permita a otras aplicaciones o una página web enviar datos para realizar predicciones. Ejemplo de creación de una API para un modelo de ML usando Python y Flask: [https://www.geeksforgeeks.org/](https://www.geeksforgeeks.org/deploy-machine-learning-model-using-flask/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
