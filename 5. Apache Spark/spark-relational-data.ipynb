{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "\n",
    "Para el procesamiento y operaciones de Big Data, la primer tarea es la recuperación de la información, entendida como la forma en que la información deseada por el usuario es especificada y recuperada del sistema del almacenamiento.\n",
    "\n",
    "Esta operación se realiza a través de consultas, las cuales utilizan un lenguaje (*Query Language*) que permite especificar la información deseada; estos lenguajes son **declarativos**, es decir, el usuario indica qué necesita en vez de como obtenerlo. No es necesario escribir un programa que indique que archivo se va a abrir, qué bytes tener en cuenta, cuáles no, el tipo de codificación, entre otras.\n",
    "\n",
    "Iniciaremos revisando la recuperación de datos relacionales, donde el lenguaje más utilizado es *Structured Query Language* o SQL con el cual se definen consultas de la forma: \n",
    "```sql\n",
    "SELECT  atributos\n",
    "FROM    tablas\n",
    "WHERE   condiciones\n",
    "```\n",
    "Donde los atributos son los nombres de las columnas de la tabla.\n",
    "\n",
    "En relación a lo visto en los modelos de datos, y las operaciones con conjuntos de datos, podríamos ver este tipo de consultas como una selección de los elementos de la tabla que cumplen cierta condición, y la posterior proyección de los atributos deseados.\n",
    "\n",
    "En este notebook veremos las dos formas que plantea el framework de Apache Spark para la recuperación de información para datos relacionales: los DataFrames y SparkSQL."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational data wrangling with Apache Spark\n",
    "\n",
    "<small>Adapted from [GitHub](https://github.com/weberdavid/pyspark_course/)</small>\n",
    "\n",
    "## Google Colaboratory environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# Next, we will install Apache Spark 3.0.1 with Hadoop 2.7 from here.\n",
    "!wget https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "# Now, we just need to unzip that folder.\n",
    "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "\n",
    "# Setting JVM and Spark path variables\n",
    "import os \n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
    "\n",
    "# Installing required packages\n",
    "!pip install pyspark==3.3.2\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as fct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark DataFrames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session and import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = (SparkSession\n",
    "      .builder\n",
    "      .appName(\"wrangling_with_data\")\n",
    "      .getOrCreate())\n",
    "\n",
    "path = \"../Data/sparkify-log.json\"\n",
    "user_data = ss.read.json(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information\n",
    "user_data.printSchema()\n",
    "(user_data\n",
    " .describe() # describe = variable + datatype; takes variable as parameter\n",
    " .show()) # show = count, mean, stddev, min, max for each var  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts number of users\n",
    "user_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects variable/column and drops duplicates\n",
    "(user_data\n",
    " .select(\"page\")\n",
    " .dropDuplicates()\n",
    " .sort(fct.desc(\"page\"))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific variables from a single user\n",
    "(user_data\n",
    " .select([\"userID\", \"firstname\", \"lastname\", \"level\"])\n",
    " .where(user_data.userId == \"1046\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating stuff with user defined function\n",
    "get_hour = fct.udf(lambda x: dt.datetime.fromtimestamp(x/1000.0).hour)\n",
    "# Create new column \"hour\" and fill with calculated hour of timestamp\n",
    "user_data = user_data.withColumn(\"hour\", get_hour(user_data.ts))\n",
    "# Show first rows\n",
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many songs are listened per hour?\n",
    "songs_in_hour = (user_data\n",
    "                 .filter(user_data.page == \"NextSong\")\n",
    "                 .groupby(user_data.hour)\n",
    "                 .count()\n",
    "                 .orderBy(user_data.hour.cast(\"float\")))\n",
    "songs_in_hour.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas df for visualization\n",
    "songs_in_hour_pd = songs_in_hour.toPandas()\n",
    "# Convert hour to numeric\n",
    "songs_in_hour_pd.hour = pd.to_numeric(songs_in_hour_pd.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "plt.scatter(songs_in_hour_pd[\"hour\"], songs_in_hour_pd[\"count\"])\n",
    "plt.xlim(-1, 24);\n",
    "plt.ylim(0, 1.2 * max(songs_in_hour_pd[\"count\"]))\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Count of Songs Played\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for and drop missing values: only NAs in userId and or sessionId\n",
    "valid_users = user_data.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "valid_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates: drop Dup, sort after User ID\n",
    "(valid_users\n",
    " .select(\"userId\")\n",
    " .dropDuplicates()\n",
    " .sort(\"userId\")\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty strings\n",
    "valid_users = valid_users.filter(valid_users[\"userId\"] != \"\")\n",
    "valid_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there users downgrading accounts?\n",
    "valid_users.filter(valid_users[\"page\"] == \"Submit Downgrade\").show()\n",
    "\n",
    "# Give downgraders a flag; first create function; second give flag to all users\n",
    "flag_downgrade_event = fct.udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n",
    "# withColumn = creates new column, or takes current and pastes values in\n",
    "valid_users = valid_users.withColumn(\"downgraded\", flag_downgrade_event(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with Window\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(fct.desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "# New column phase, that is the sum of downgraded\n",
    "valid_users = valid_users.withColumn(\"phase\", fct.sum(\"downgraded\").over(windowval))\n",
    "# Select variables of user, sort and collect data\n",
    "(valid_users\n",
    " .select([\"userId\", \"firstname\", \"ts\", \"page\", \"level\", \"phase\"])\n",
    " .where(user_data.userId == \"1138\")\n",
    " .sort(\"ts\")\n",
    " .collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view to run SQL queries\n",
    "user_data.createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql('''\n",
    "       SELECT *\n",
    "       FROM user_data_table\n",
    "       LIMIT 2\n",
    "       ''').show()  # .show is required to surpass lazyevaluation of spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ss.sql('''\n",
    "       SELECT userId, count(page)\n",
    "       FROM user_data_table\n",
    "       GROUP BY userId\n",
    "       ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql('''\n",
    "       SELECT userId, firstname, page, song\n",
    "       FROM user_data_table\n",
    "       WHERE userId = '1046'\n",
    "       ''').collect()  # Attention - difference between show and collect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be registered first\n",
    "ss.udf.register(\"get_hour\", lambda x: int(dt.datetime.fromtimestamp(x / 1000).hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql('''\n",
    "       SELECT userId, AVG(get_hour(ts)) as avg_hour\n",
    "       FROM user_data_table\n",
    "       GROUP BY userId\n",
    "       ''').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which page did user id \"\" NOT visit?\n",
    "\n",
    "Double-click for one possible solution.\n",
    "\n",
    "<!--\n",
    "(user_data\n",
    " .where(user_data[\"userId\"] == \"\")\n",
    " .groupby(user_data.page)\n",
    " .count()\n",
    " .show())\n",
    "\n",
    "ss.sql('''\n",
    "       SELECT page\n",
    "       FROM user_data_table\n",
    "       WHERE userId = \"\"\n",
    "       GROUP BY page\n",
    "       ''').show()\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many female users are in the dataset?\n",
    "\n",
    "Double-click for one possible solution.\n",
    "<!--\n",
    "(user_data\n",
    " .select([\"userId\"])\n",
    " .dropDuplicates()\n",
    " .where(user_data[\"gender\"] == \"F\")\n",
    " .count())\n",
    "\n",
    "ss.sql('''\n",
    "       SELECT count(distinct(userId))\n",
    "       FROM user_data_table\n",
    "       WHERE gender = 'F'\n",
    "       ''').show()\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the most played artist, how many songs were played?\n",
    "Double-click for one possible solution.\n",
    "<!--\n",
    "(user_data\n",
    " .filter(user_data.page == \"NextSong\")\n",
    " .select(\"Artist\")\n",
    " .groupby(\"Artist\")\n",
    " .count()\n",
    " .sort(fct.desc(\"count\"))\n",
    " .show(1))\n",
    "\n",
    "ss.sql('''\n",
    "       SELECT artist, count(artist) as count\n",
    "       FROM user_data_table\n",
    "       WHERE page = 'NextSong'\n",
    "       GROUP BY artist\n",
    "       ORDER BY count DESC\n",
    "       LIMIT 1\n",
    "       ''').show()\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
